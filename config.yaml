# DeepSeek R1 Zero GRPO Training Configuration
# This is the centralized configuration file for the DeepSeek R1 Zero reproduction project.

# Project metadata
project:
  name: "deepseek-r1-zero-grpo"
  version: "1.0.0"
  description: "DeepSeek R1 Zero reproduction with GRPO training"

# Model configuration
model:
  name: "Qwen/Qwen2.5-7B"
  revision: "main"
  torch_dtype: "bfloat16"
  trust_remote_code: true
  attn_implementation: "flash_attention_2"
  device_placement: "auto"  # auto, single, multi
  
  # Unsloth FastLanguageModel configuration
  unsloth:
    enabled: false
    load_in_4bit: true
    fast_inference: true
    gpu_memory_utilization: 0.85
  
  # LoRA PEFT configuration
  lora:
    enabled: true
    rank: 32
    alpha: 32
    target_modules: ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
    use_gradient_checkpointing: "unsloth"
    random_state: 3407

# Training Arguments Configuration - exact parameter names from transformers.TrainingArguments
TrainingArguments:
  output_dir: null  # auto-generated if null
  overwrite_output_dir: false
  num_train_epochs: 1.0
  max_steps: 10000  # -1 means use num_train_epochs instead
  per_device_train_batch_size: 16
  per_device_eval_batch_size: 32
  gradient_accumulation_steps: 1
  learning_rate: 0.00005
  warmup_ratio: 0.05
  weight_decay: 0.01
  logging_steps: 10
  eval_strategy: "no"
  eval_steps: 200
  save_strategy: "steps"
  save_steps: 50
  save_total_limit: 2
  dataloader_num_workers: 0
  dataloader_pin_memory: false
  dataloader_persistent_workers: false
  dataloader_prefetch_factor: 2
  dataloader_drop_last: true
  seed: 42
  bf16: true
  tf32: true
  gradient_checkpointing: false
  push_to_hub: false
  report_to: "none"
  remove_unused_columns: false
  group_by_length: true
  torch_compile: false
  
  # Optimizer configuration
  optim: "adamw_torch"  # Use adamw_8bit for memory efficiency
  adam_beta1: 0.9       # Default: 0.9, user example: 0.9
  adam_beta2: 0.999     # Default: 0.999, user example: 0.99
  max_grad_norm: 1.0    # Default: 1.0, user example: 0.1
  
  # Learning rate scheduler
  lr_scheduler_type: "linear"  # Default: linear, user example: cosine

# Dataset configuration
dataset:
  name: "Jiayi-Pan/Countdown-Tasks-3to4"
  subset: null
  
  # Dataset splitting
  split:
    test_size: 128
    seed: 42
  
  # Data processing
  processing:
    max_length: 2048
    system_prompt: "A conversation between User and Assistant. The user asks a question, and the Assistant solves it. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think><answer> answer here </answer>"

# GRPO Configuration - additional parameters beyond TrainingArguments
GRPOConfig:
  max_prompt_length: 256      # User example: 256
  max_completion_length: 1024 # User example: 200
  num_generations: 8
  use_vllm: true
  vllm_mode: "colocate"
  vllm_gpu_memory_utilization: 0.3
  use_liger_loss: false
  log_completions: true
  wandb_log_unique_prompts: true
  ddp_find_unused_parameters: false

# Reward function configuration
rewards:
  functions: ["format", "equation"]
  
  # Cosine reward parameters
  cosine:
    min_value_wrong: -0.5
    max_value_wrong: -0.1
    min_value_correct: 0.8
    max_value_correct: 1.0
    max_len: 1000
  
  # Repetition penalty parameters
  repetition:
    n_grams: 3
    max_penalty: -0.1
  
  # Code reward parameters
  code:
    language: "python"
  
  # Soft punishment parameters
  soft_punish:
    max_completion_len: 512
    cache: 50

# System configuration
system:
  output_dir: null  # auto-generated if null

# Monitoring and logging
monitoring:
  # Weights & Biases
  wandb:
    enabled: true
    project: "deepseek-r1-zero-grpo"
    run_name: null
  
  # Logging configuration
  logging:
    level: "WARNING"
    file: "training.log"
    profiling_mode: false

# Callback configuration
callbacks:
  # Comprehensive logging callback
  comprehensive_logging:
    enabled: false
    log_examples: true
  
  # Reward trend analysis
  reward_trend:
    window_size: 50
  
  # Checkpoint preservation
  checkpoint_preservation:
    enabled: true
    every_n_steps: 2000
    directory: "permanent_checkpoints"

# Configuration profiles for different environments
_profiles:
  # Development profile - faster training, less logging
  dev:
    TrainingArguments.num_train_epochs: 1.0
    TrainingArguments.per_device_train_batch_size: 8
    TrainingArguments.logging_steps: 5
    TrainingArguments.save_steps: 25
    monitoring.wandb.enabled: false
    monitoring.logging.profiling_mode: false
    callbacks.checkpoint_preservation.enabled: false
    model.name: "Qwen/Qwen2.5-0.5B-Instruct"
    GRPOConfig.use_vllm: true
  
  # Production profile - full training, comprehensive logging
  prod:
    TrainingArguments.num_train_epochs: 3.0
    TrainingArguments.per_device_train_batch_size: 8
    TrainingArguments.learning_rate: 3.0e-5
    monitoring.wandb.enabled: true
    monitoring.logging.profiling_mode: false
    callbacks.comprehensive_logging.enabled: false
    callbacks.checkpoint_preservation.enabled: true
    callbacks.checkpoint_preservation.every_n_steps: 1000
    model.name: "Qwen/Qwen2.5-7B-Instruct"
  
  # Testing profile - minimal resources, quick validation
  debug:
    model.name: "Qwen/Qwen2.5-0.5B-Instruct"
    model.attn_implementation: "eager"
    model.unsloth.gpu_memory_utilization: 0.3
    TrainingArguments.num_train_epochs: 0.01
    TrainingArguments.per_device_train_batch_size: 1
    TrainingArguments.gradient_accumulation_steps: 1
    TrainingArguments.per_device_eval_batch_size: 1
    TrainingArguments.logging_steps: 1
    monitoring.wandb.enabled: false
    callbacks.checkpoint_preservation.enabled: false
    dataset.split.test_size: 128
    rewards.functions: ["format"]
    log_completions: false
    wandb_log_unique_prompts: false
  
  # Profiling profile - detailed performance analysis
  profile:
    TrainingArguments.num_train_epochs: 0.1
    TrainingArguments.per_device_train_batch_size: 8
    TrainingArguments.logging_steps: 1
    monitoring.wandb.enabled: false
    monitoring.logging.profiling_mode: true
    callbacks.comprehensive_logging.enabled: true
    callbacks.comprehensive_logging.log_examples: true
    callbacks.checkpoint_preservation.enabled: false
    model.name: "Qwen/Qwen2.5-0.5B-Instruct"